import os
import sys
import time
import random
import requests
from pathlib import Path
from typing import List
from duckduckgo_search import DDGS
from dotenv import load_dotenv, find_dotenv

# --- 1. ç’°å¢ƒè¨­å®š ---
env_path = find_dotenv()
if env_path:
    load_dotenv(env_path)

API_KEY = os.getenv("GOOGLE_API_KEY")
SEARCH_ENGINE_ID = os.getenv("GOOGLE_SEARCH_ENGINE_ID")
BASE_DIR = Path("./data/ingredients")


# --- 2. å…±é€š: ç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–¢æ•° ---
def download_images(urls: List[str], save_dir: Path, prefix: str):
    save_dir.mkdir(parents=True, exist_ok=True)
    success_count = 0

    # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã®æšæ•°ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆæ—¢ã«50æšã‚ã‚‹ãªã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãªã„ï¼‰
    existing = len(list(save_dir.glob("*.*")))
    if existing >= 50:
        print(f"â© {prefix} ã¯æ—¢ã« {existing} æšã‚ã‚‹ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return

    print(f"ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹: {len(urls)} ä»¶ -> {save_dir}")

    for i, url in enumerate(urls):
        try:
            res = requests.get(url, timeout=10)
            res.raise_for_status()

            ext = "jpg"
            if ".png" in url.lower():
                ext = "png"
            elif ".jpeg" in url.lower():
                ext = "jpeg"

            timestamp = int(time.time())
            # ãƒ•ã‚¡ã‚¤ãƒ«åé‡è¤‡é˜²æ­¢ã®å·¥å¤«
            filename = f"{prefix}_{success_count + 1 + existing:03d}_{timestamp}.{ext}"
            save_path = save_dir / filename

            with open(save_path, "wb") as f:
                f.write(res.content)

            success_count += 1
            # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–“éš”ã‚‚å°‘ã—ãƒ©ãƒ³ãƒ€ãƒ ã«ã™ã‚‹
            time.sleep(random.uniform(0.5, 1.5))

        except Exception:
            pass

    print(f"ğŸ‰ å®Œäº†: ä»Šå› {success_count} æšä¿å­˜ (åˆè¨ˆ {existing + success_count} æš)")


# --- 3. Googleæ¤œç´¢ (ç¢ºå®Ÿãª10æš) ---
def fetch_google_urls(query: str, count: int = 10) -> List[str]:
    if not API_KEY:
        return []

    print(f"ğŸ¤– [Google] '{query}' ã‚’æ¤œç´¢ä¸­...")
    search_url = "https://www.googleapis.com/customsearch/v1"
    params = {
        "q": query,
        "key": API_KEY,
        "cx": SEARCH_ENGINE_ID,
        "searchType": "image",
        "num": 10,
        "start": 1,
        "safe": "off",
    }

    try:
        res = requests.get(search_url, params=params, timeout=10)
        res.raise_for_status()
        items = res.json().get("items", [])
        return [item["link"] for item in items][:count]
    except Exception as e:
        print(f"âŒ [Google] ã‚¨ãƒ©ãƒ¼: {e}")
        return []


# --- 4. DuckDuckGoæ¤œç´¢ (ã‚¹ãƒ†ãƒ«ã‚¹ä»•æ§˜) ---
def fetch_ddg_urls(query: str, count: int) -> List[str]:
    if count <= 0:
        return []

    #æ¤œç´¢å‰ã«ã—ã£ã‹ã‚Šä¼‘æ†©ã™ã‚‹
    sleep_time = random.uniform(5, 10)
    print(f"ğŸ’¤ DDGè­¦æˆ’å›é¿ã®ãŸã‚ {sleep_time:.1f} ç§’å¾…æ©Ÿä¸­...")
    time.sleep(sleep_time)

    print(f"ğŸ¦† [DuckDuckGo] '{query}' ã‚’æ¤œç´¢ä¸­... (ç›®æ¨™: {count}æš)")
    urls = []

    try:
        with DDGS() as ddgs:
            # max_resultsã‚’æŒ‡å®šã—ã¦å–å¾—
            results = ddgs.images(keywords=query, max_results=count)
            urls = [r["image"] for r in results]
    except Exception as e:
        print(f"âŒ [DuckDuckGo] å–å¾—å¤±æ•—: {e}")
        print("   -> ç„¡ç†ã›ãšGoogleã®åˆ†ã ã‘ã§é€²ã¿ã¾ã™ã€‚")

    return urls


# --- 5. ãƒ¡ã‚¤ãƒ³å‡¦ç† (20é£Ÿæå¯¾å¿œãƒ«ãƒ¼ãƒ—) ---
def process_ingredients(target_list: List[str]):
    print(f"ğŸ“‹ å…¨ {len(target_list)} é£Ÿæã®å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™ã€‚")

    for i, target in enumerate(target_list):
        print(f"\n[{i+1}/{len(target_list)}] Target: {target} " + "="*20)

        all_urls = []

        # 1. Google (å¿…ãšå®Ÿè¡Œ)
        google_urls = fetch_google_urls(target, count=10)
        all_urls.extend(google_urls)
        print(f"   -> Google: {len(google_urls)} ä»¶")

        # 2. DDG (Googleã§å–ã‚ŒãŸåˆ†ã‚’å·®ã—å¼•ã„ã¦å®Ÿè¡Œ)
        remaining = 50 - len(all_urls)
        if remaining > 0:
            ddg_urls = fetch_ddg_urls(target, count=remaining)
            all_urls.extend(ddg_urls)
            print(f"   -> DDG: {len(ddg_urls)} ä»¶")

        # 3. ä¿å­˜
        unique_urls = list(set(all_urls))
        save_dir = BASE_DIR / target
        download_images(unique_urls, save_dir, target)

        # â˜…é£Ÿæã¨é£Ÿæã®é–“ã«ã‚‚é•·ã„ä¼‘æ†©ã‚’å…¥ã‚Œã‚‹
        if i < len(target_list) - 1:
            rest_time = random.uniform(10, 20)
            print(f"â˜• æ¬¡ã®é£Ÿæã¾ã§ {rest_time:.1f} ç§’ä¼‘æ†©ã—ã¾ã™...")
            time.sleep(rest_time)


if __name__ == "__main__":
    # --- ã“ã“ã«20ç¨®é¡ä»¥ä¸Šã®é£Ÿæãƒªã‚¹ãƒˆã‚’æ›¸ã„ã¦ãã ã•ã„ ---
    ingredients_list = [
        "ãƒ‘ã‚¯ãƒãƒ¼",
        "ãƒˆãƒãƒˆ",
        "ãã‚…ã†ã‚Š",
        "ã‚­ãƒ£ãƒ™ãƒ„",
        "ç‰ã­ã",
        "ã˜ã‚ƒãŒã„ã‚‚",
        "äººå‚",
        "å¤§æ ¹",
        "ãªã™",
        "ãƒ”ãƒ¼ãƒãƒ³",
        # ... ä»–ã®é£Ÿæã‚’è¿½åŠ  ...
    ]

    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ãŒã‚ã‚Œã°ãã‚Œã‚’å„ªå…ˆã€ãªã‘ã‚Œã°ãƒªã‚¹ãƒˆã‚’å®Ÿè¡Œ
    if len(sys.argv) > 1:
        process_ingredients(sys.argv[1:])
    else:
        process_ingredients(ingredients_list)
