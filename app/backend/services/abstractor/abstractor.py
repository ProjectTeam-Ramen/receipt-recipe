import sys
import time
import random
import urllib.request
import urllib.parse
import urllib.error
import json
from pathlib import Path
from typing import List
from dotenv import load_dotenv, find_dotenv
import os

# --- 1. ç’°å¢ƒè¨­å®š ---
env_path = find_dotenv()
if env_path:
    load_dotenv(env_path)

GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
SEARCH_ENGINE_ID = os.getenv("GOOGLE_SEARCH_ENGINE_ID")

BASE_DIR = Path("./data/ingredients")
GOOGLE_SEARCH_URL = "https://www.googleapis.com/customsearch/v1"


# --- 2. å…±é€š: ç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–¢æ•° ---
def download_images(urls: List[str], save_dir: Path, prefix: str, target_count: int):
    save_dir.mkdir(parents=True, exist_ok=True)
    success_count = 0
    existing = len(list(save_dir.glob("*.*")))

    if existing >= target_count:
        print(f"â© {prefix} ã¯æ—¢ã« {existing} æšã‚ã‚‹ãŸã‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return

    print(f"ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹: {len(urls)} ä»¶ -> {save_dir}")

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    for i, url in enumerate(urls):
        if existing + success_count >= target_count:
            break

        try:
            req = urllib.request.Request(url, headers=headers)
            with urllib.request.urlopen(req, timeout=10) as response:
                content = response.read()

                ext = "jpg"
                if ".png" in url.lower():
                    ext = "png"
                elif ".jpeg" in url.lower():
                    ext = "jpeg"
                elif ".gif" in url.lower():
                    ext = "gif"

                timestamp = int(time.time())
                filename = (
                    f"{prefix}_{success_count + 1 + existing:03d}_{timestamp}.{ext}"
                )
                save_path = save_dir / filename

                with open(save_path, "wb") as f:
                    f.write(content)

                success_count += 1
                time.sleep(random.uniform(0.5, 1.0))

        except Exception:
            pass

    print(f"ğŸ‰ å®Œäº†: ä»Šå› {success_count} æšä¿å­˜ (åˆè¨ˆ {existing + success_count} æš)")


# --- 3. Google Custom Search API æ¤œç´¢ãƒ­ã‚¸ãƒƒã‚¯ ---
def fetch_google_image_urls(query: str, count: int) -> List[str]:
    if not GOOGLE_API_KEY or not SEARCH_ENGINE_ID:
        print("âŒ APIã‚­ãƒ¼ã¾ãŸã¯æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³IDãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return []

    print(f"ğŸ” [Google API] '{query}' ã‚’æ¤œç´¢ä¸­... (ç›®æ¨™: {count}æš)")

    urls = []
    start_index = 1

    # 10æšä»¥ä¸‹ãªã‚‰1å›ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã§æ¸ˆã‚€ãŸã‚ã€ãƒ«ãƒ¼ãƒ—æ¡ä»¶ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ã«ãªã‚Šã¾ã™
    while len(urls) < count:
        params = {
            "q": query,
            "key": GOOGLE_API_KEY,
            "cx": SEARCH_ENGINE_ID,
            "searchType": "image",
            "num": 10,  # MAX10ä»¶
            "start": start_index,
            "safe": "off",
            "fileType": "jpg",
        }

        query_string = urllib.parse.urlencode(params)
        request_url = f"{GOOGLE_SEARCH_URL}?{query_string}"

        try:
            with urllib.request.urlopen(request_url, timeout=15) as res:
                data = json.loads(res.read().decode("utf-8"))

                items = data.get("items", [])
                if not items:
                    print("âš ï¸ ã“ã‚Œä»¥ä¸Šã®çµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                    break

                for item in items:
                    link = item.get("link")
                    if link:
                        urls.append(link)

                # APIåˆ¶é™: startãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ä¸Šé™ãªã©ã‚’è€ƒæ…®ã—ã¤ã¤æ¬¡ã¸
                start_index += 10

                # countãŒ10ä»¥ä¸‹ã®å ´åˆã¯1å›ã§breakã—ã¦ç„¡é§„ãªãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é˜²ã
                if count <= 10:
                    break

                time.sleep(1)

        except urllib.error.HTTPError as e:
            print(f"âŒ APIãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e.code} - {e.reason}")
            if e.code == 403:
                print("   -> APIã®åˆ©ç”¨æ ä¸Šé™ã€ã¾ãŸã¯ã‚­ãƒ¼ã®è¨­å®šãƒŸã‚¹ã§ã™ã€‚")
            break
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            break

    return urls[:count]


# --- 4. ãƒ¡ã‚¤ãƒ³å‡¦ç† ---
def process_ingredients(target_list: List[str]):
    # â˜… ã“ã“ã‚’50ã‹ã‚‰10ã«å¤‰æ›´ã—ã¾ã—ãŸ â˜…
    TARGET_COUNT = 10

    print(
        f"ğŸ“‹ å…¨ {len(target_list)} é£Ÿæã®å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™ (ç›®æ¨™: å„{TARGET_COUNT}æš)ã€‚"
    )

    for i, target in enumerate(target_list):
        print(f"\n[{i + 1}/{len(target_list)}] Target: {target} " + "=" * 20)

        save_dir = BASE_DIR / target
        save_dir.mkdir(parents=True, exist_ok=True)
        existing = len(list(save_dir.glob("*.*")))
        needed = TARGET_COUNT - existing

        if needed <= 0:
            print(f"â© {target} ã¯æ—¢ã« {existing} æšã‚ã‚‹ãŸã‚æ¤œç´¢ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        else:
            g_urls = fetch_google_image_urls(target, count=needed)
            print(f" Â  -> å–å¾—URLæ•°: {len(g_urls)} ä»¶")

            if g_urls:
                download_images(g_urls, save_dir, target, TARGET_COUNT)
            else:
                print(" Â  -> ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

        if i < len(target_list) - 1:
            time.sleep(1)  # å¾…æ©Ÿæ™‚é–“ã‚‚å°‘ã—çŸ­ç¸®


if __name__ == "__main__":
    ingredients_list = [
        "tomato",
        "cucumber",
        "onion",
        "carrot",
        # ...
    ]

    if len(sys.argv) > 1:
        process_ingredients(sys.argv[1:])
    else:
        process_ingredients(ingredients_list)
