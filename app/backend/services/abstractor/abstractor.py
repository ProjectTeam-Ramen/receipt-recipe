import argparse
import json
import os
import random
import time
import urllib.error
import urllib.parse
import urllib.request
from pathlib import Path
from typing import List, Optional

from dotenv import find_dotenv, load_dotenv

# --- 1. ç’°å¢ƒè¨­å®š ---
env_path = find_dotenv()
if env_path:
    load_dotenv(env_path)

GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
SEARCH_ENGINE_ID = os.getenv("GOOGLE_SEARCH_ENGINE_ID")

BASE_DIR = Path("./data/ingredients")
GOOGLE_SEARCH_URL = "https://www.googleapis.com/customsearch/v1"


# --- 2. å…±é€š: ç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–¢æ•° ---
def download_images(urls: List[str], save_dir: Path, prefix: str, target_count: int):
    save_dir.mkdir(parents=True, exist_ok=True)
    success_count = 0
    existing = len(list(save_dir.glob("*.*")))

    if existing >= target_count:
        print(f"â© {prefix} ã¯æ—¢ã« {existing} æšã‚ã‚‹ãŸã‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return

    print(f"ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹: {len(urls)} ä»¶ -> {save_dir}")

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    for i, url in enumerate(urls):
        if existing + success_count >= target_count:
            break

        try:
            req = urllib.request.Request(url, headers=headers)
            with urllib.request.urlopen(req, timeout=10) as response:
                content = response.read()

                ext = "jpg"
                if ".png" in url.lower():
                    ext = "png"
                elif ".jpeg" in url.lower():
                    ext = "jpeg"
                elif ".gif" in url.lower():
                    ext = "gif"

                timestamp = int(time.time())
                filename = (
                    f"{prefix}_{success_count + 1 + existing:03d}_{timestamp}.{ext}"
                )
                save_path = save_dir / filename

                with open(save_path, "wb") as f:
                    f.write(content)

                success_count += 1
                time.sleep(random.uniform(0.5, 1.0))

        except Exception:
            pass

    print(f"ğŸ‰ å®Œäº†: ä»Šå› {success_count} æšä¿å­˜ (åˆè¨ˆ {existing + success_count} æš)")


def download_single_image(url: str, save_dir: Path, prefix: str) -> Optional[Path]:
    save_dir.mkdir(parents=True, exist_ok=True)
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }
    try:
        req = urllib.request.Request(url, headers=headers)
        with urllib.request.urlopen(req, timeout=10) as response:
            content = response.read()
    except Exception as exc:  # pragma: no cover - depends on network
        print(f"âŒ ç”»åƒã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {exc}")
        return None

    parsed_path = Path(urllib.parse.urlparse(url).path)
    ext = parsed_path.suffix.lower() or ".jpg"
    if ext not in {".jpg", ".jpeg", ".png", ".gif"}:
        ext = ".jpg"

    filename = f"{prefix}_{int(time.time() * 1000)}{ext}"
    save_path = save_dir / filename
    with open(save_path, "wb") as handle:
        handle.write(content)

    print(f"ğŸ“¸ ç”»åƒã‚’ä¿å­˜ã—ã¾ã—ãŸ: {save_path}")
    return save_path


# --- 3. Google Custom Search API æ¤œç´¢ãƒ­ã‚¸ãƒƒã‚¯ ---
def fetch_google_image_urls(query: str, count: int) -> List[str]:
    if not GOOGLE_API_KEY or not SEARCH_ENGINE_ID:
        print("âŒ APIã‚­ãƒ¼ã¾ãŸã¯æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³IDãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return []

    print(f"ğŸ” [Google API] '{query}' ã‚’æ¤œç´¢ä¸­... (ç›®æ¨™: {count}æš)")

    urls = []
    start_index = 1

    # 10æšä»¥ä¸‹ãªã‚‰1å›ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã§æ¸ˆã‚€ãŸã‚ã€ãƒ«ãƒ¼ãƒ—æ¡ä»¶ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ã«ãªã‚Šã¾ã™
    while len(urls) < count:
        params = {
            "q": query,
            "key": GOOGLE_API_KEY,
            "cx": SEARCH_ENGINE_ID,
            "searchType": "image",
            "num": 10,  # MAX10ä»¶
            "start": start_index,
            "safe": "off",
            "fileType": "jpg",
        }

        query_string = urllib.parse.urlencode(params)
        request_url = f"{GOOGLE_SEARCH_URL}?{query_string}"

        try:
            with urllib.request.urlopen(request_url, timeout=15) as res:
                data = json.loads(res.read().decode("utf-8"))

                items = data.get("items", [])
                if not items:
                    print("âš ï¸ ã“ã‚Œä»¥ä¸Šã®çµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                    break

                for item in items:
                    link = item.get("link")
                    if link:
                        urls.append(link)

                # APIåˆ¶é™: startãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ä¸Šé™ãªã©ã‚’è€ƒæ…®ã—ã¤ã¤æ¬¡ã¸
                start_index += 10

                # countãŒ10ä»¥ä¸‹ã®å ´åˆã¯1å›ã§breakã—ã¦ç„¡é§„ãªãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é˜²ã
                if count <= 10:
                    break

                time.sleep(1)

        except urllib.error.HTTPError as e:
            print(f"âŒ APIãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e.code} - {e.reason}")
            if e.code == 403:
                print("   -> APIã®åˆ©ç”¨æ ä¸Šé™ã€ã¾ãŸã¯ã‚­ãƒ¼ã®è¨­å®šãƒŸã‚¹ã§ã™ã€‚")
            break
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            break

    return urls[:count]


def recognize_targets(target_list: List[str], *, top_k: int = 5) -> None:
    if not target_list:
        print("âš ï¸ åˆ¤å®šå¯¾è±¡ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return

    from app.backend.services.item_abstractor.image_recognition.image_recognizer_predict import (  # noqa: PLC0415
        get_top_predictions,
        predict_image,
    )

    for target in target_list:
        print(f"\nğŸ” '{target}' ã®ç”»åƒã‚’æ¤œç´¢ã—ã€åˆ¤å®šã—ã¾ã™ã€‚")
        urls = fetch_google_image_urls(target, count=1)
        if not urls:
            print("   -> ç”»åƒURLãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
            continue

        save_dir = BASE_DIR / "recognized" / target.replace(" ", "_")
        image_path = download_single_image(urls[0], save_dir, target)
        if not image_path:
            continue

        try:
            probabilities = predict_image(image_path)
            top_results = get_top_predictions(probabilities, top_k)
            print("   -> åˆ¤å®šçµæœ:")
            for label, score in top_results:
                print(f"      {label}: {score:.3f}")
        except Exception as exc:  # pragma: no cover - mainly runtime errors
            print(f"   -> åˆ¤å®šã«å¤±æ•—ã—ã¾ã—ãŸ: {exc}")


# --- 4. ãƒ¡ã‚¤ãƒ³å‡¦ç† ---
def process_ingredients(target_list: List[str]):
    TARGET_COUNT = 10

    print(
        f"ğŸ“‹ å…¨ {len(target_list)} é£Ÿæã®å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™ (ç›®æ¨™: å„{TARGET_COUNT}æš)ã€‚"
    )

    for i, target in enumerate(target_list):
        print(f"\n[{i + 1}/{len(target_list)}] Target: {target} " + "=" * 20)

        save_dir = BASE_DIR / target
        save_dir.mkdir(parents=True, exist_ok=True)
        existing = len(list(save_dir.glob("*.*")))
        needed = TARGET_COUNT - existing

        if needed <= 0:
            print(f"â© {target} ã¯æ—¢ã« {existing} æšã‚ã‚‹ãŸã‚æ¤œç´¢ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        else:
            g_urls = fetch_google_image_urls(target, count=needed)
            print(f" Â  -> å–å¾—URLæ•°: {len(g_urls)} ä»¶")

            if g_urls:
                download_images(g_urls, save_dir, target, TARGET_COUNT)
            else:
                print(" Â  -> ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

        if i < len(target_list) - 1:
            time.sleep(1)  # å¾…æ©Ÿæ™‚é–“ã‚‚å°‘ã—çŸ­ç¸®


def main(argv: Optional[List[str]] = None) -> None:
    parser = argparse.ArgumentParser(
        description="é£Ÿæç”»åƒã®å–å¾—ãŠã‚ˆã³åˆ†é¡ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£"
    )
    parser.add_argument("targets", nargs="*", help="å‡¦ç†ã—ãŸã„é£Ÿæå")
    parser.add_argument(
        "--bulk",
        action="store_true",
        help="å¾“æ¥ã©ãŠã‚Šã€å¯¾è±¡é£Ÿæã”ã¨ã«ç”»åƒã‚’10æšåé›†ã™ã‚‹ãƒ¢ãƒ¼ãƒ‰",
    )
    parser.add_argument(
        "--top",
        type=int,
        default=5,
        help="åˆ¤å®šæ™‚ã«è¡¨ç¤ºã™ã‚‹ä¸Šä½ä»¶æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5)",
    )
    args = parser.parse_args(argv)

    default_targets = [
        "tomato",
        "cucumber",
        "onion",
        "carrot",
    ]

    targets = args.targets or default_targets

    if args.bulk:
        process_ingredients(targets)
    else:
        recognize_targets(targets, top_k=args.top)


if __name__ == "__main__":
    main()
